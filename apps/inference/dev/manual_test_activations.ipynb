{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records(\n",
    "    {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}\n",
    ").T\n",
    "df.drop(\n",
    "    columns=[\n",
    "        \"expected_var_explained\",\n",
    "        \"expected_l0\",\n",
    "        \"config_overrides\",\n",
    "        \"conversion_func\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "df.query(\"model == 'gemma-2-2b'\")\n",
    "# Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# sae_1,_,_ = SAE.from_pretrained(\"gemma-scope-2b-pt-mlp-canonical\",\"layer_0/width_65k/canonical\", device=\"mps\")\n",
    "sae_1, _, _ = SAE.from_pretrained(\n",
    "    \"gpt2-small-res-jb\", \"blocks.11.hook_resid_post\", device=\"mps\"\n",
    ")\n",
    "# model = HookedSAETransformer.from_pretrained(sae_1.cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import plotly.express as px\n",
    "from sae_lens.analysis.neuronpedia_integration import (\n",
    "    open_neuronpedia_feature_dashboard,\n",
    "    get_neuronpedia_quick_list,\n",
    ")\n",
    "\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained_no_processing(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=True,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=False,\n",
    ")\n",
    "\n",
    "sae_1, _, _ = SAE.from_pretrained(\n",
    "    \"gpt2-small-res-jb\", \"blocks.11.hook_resid_post\", device=\"mps\"\n",
    ")\n",
    "\n",
    "test_text = \"\"\" outscored the Pacers by just 16 points in the series. There has never been a smaller margin in a four-game sweep. — Dan Feldman (@DanFeldmanNBA) April 23, 2017\n",
    "\n",
    "Why did it play out this way? Well, there were quite a few mitigating factors.\n",
    "\n",
    "First, the Pacers played really, really well. Paul George played like a top-five player in this series, and the Cavaliers declined to put their best defensive weapon, LeBron James on him.\n",
    "\n",
    "George averaged 28.0 points, 8.8 rebounds and 7.3 assists for the series, with his assists\"\"\"\n",
    "\n",
    "text_df = pd.DataFrame(enumerate(model.to_str_tokens(test_text)))\n",
    "text_df.columns = [\"token_id\", \"token\"]\n",
    "text_df[\"token_id\"] = text_df[\"token_id\"].astype(int)\n",
    "text_df[\"token\"] = text_df[\"token\"].astype(str)\n",
    "# find the token \"Dan\"\n",
    "display(text_df.query(\"token == 'Dan'\"))\n",
    "\n",
    "_, cache = model.run_with_cache_with_saes(test_text, saes=[sae_1])\n",
    "px.line(\n",
    "    cache[sae_1.cfg.hook_name + \".hook_sae_acts_post\"][0, 32, :].cpu().numpy()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "import plotly.express as px\n",
    "from sae_lens.analysis.neuronpedia_integration import (\n",
    "    open_neuronpedia_feature_dashboard,\n",
    "    get_neuronpedia_quick_list,\n",
    ")\n",
    "\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained_no_processing(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=True,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    ")\n",
    "\n",
    "sae_1, _, _ = SAE.from_pretrained(\n",
    "    \"gpt2-small-res-jb\", \"blocks.11.hook_resid_post\", device=\"mps\"\n",
    ")\n",
    "\n",
    "test_text = \"\"\" outscored the Pacers by just 16 points in the series. There has never been a smaller margin in a four-game sweep. — Dan Feldman (@DanFeldmanNBA) April 23, 2017\n",
    "\n",
    "Why did it play out this way? Well, there were quite a few mitigating factors.\n",
    "\n",
    "First, the Pacers played really, really well. Paul George played like a top-five player in this series, and the Cavaliers declined to put their best defensive weapon, LeBron James on him.\n",
    "\n",
    "George averaged 28.0 points, 8.8 rebounds and 7.3 assists for the series, with his assists\"\"\"\n",
    "\n",
    "text_df = pd.DataFrame(enumerate(model.to_str_tokens(test_text)))\n",
    "text_df.columns = [\"token_id\", \"token\"]\n",
    "text_df[\"token_id\"] = text_df[\"token_id\"].astype(int)\n",
    "text_df[\"token\"] = text_df[\"token\"].astype(str)\n",
    "# find the token \"Dan\"\n",
    "display(text_df.query(\"token == 'Dan'\"))\n",
    "\n",
    "_, cache = model.run_with_cache_with_saes(test_text, saes=[sae_1])\n",
    "px.line(\n",
    "    cache[sae_1.cfg.hook_name + \".hook_sae_acts_post\"][0, 32, :].cpu().numpy()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sae_1, _, _ = SAE.from_pretrained(\n",
    "        \"gemma-scope-2b-pt-mlp-canonical\",\n",
    "        f\"layer_{i}/width_65k/canonical\",\n",
    "        device=\"mps\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import plotly.express as px\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained_no_processing(\n",
    "    \"gemma-2-2b\",\n",
    "    fold_ln=True,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=False,\n",
    ")\n",
    "\n",
    "# sae_1,_,_ = SAE.from_pretrained(\"gpt2-small-res-jb\",\"blocks.11.hook_resid_post\", device=\"mps\")\n",
    "\n",
    "test_text = \"\"\" damn\"\"\"\n",
    "\n",
    "# text_df = pd.DataFrame(enumerate(model.to_str_tokens(test_text)))\n",
    "# text_df.columns = [\"token_id\", \"token\"]\n",
    "# text_df[\"token_id\"] = text_df[\"token_id\"].astype(int)\n",
    "# text_df[\"token\"] = text_df[\"token\"].astype(str)\n",
    "# # find the token \"Dan\"\n",
    "# display(text_df.query(\"token == 'Dan'\"))\n",
    "\n",
    "_, cache = model.run_with_cache_with_saes(test_text, saes=[sae_1])\n",
    "px.line(\n",
    "    cache[sae_1.cfg.hook_name + \".hook_sae_acts_post\"][0, -1, :].cpu().numpy()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import plotly.express as px\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained_no_processing(\n",
    "    \"gemma-2-2b\",\n",
    "    fold_ln=True,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    ")\n",
    "\n",
    "test_text = \"\"\" damn\"\"\"\n",
    "\n",
    "_, cache = model.run_with_cache_with_saes(test_text, saes=[sae_1])\n",
    "px.line(\n",
    "    cache[sae_1.cfg.hook_name + \".hook_sae_acts_post\"][0, -1, :].cpu().numpy()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_1.cfg.neuronpedia_id.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_1.cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_1.cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Test activations_all endpoint\n",
    "base_url = \"http://127.0.0.1:5000\"\n",
    "url = f\"{base_url}/activations-all\"\n",
    "\n",
    "payload = {\n",
    "    \"text\": test_text,\n",
    "    \"model\": \"gpt2-small\",\n",
    "    \"source_set\": \"res-jb\",\n",
    "    \"selected_layers\": [\"12-res-jb\"],\n",
    "    \"secret\": \"secret\",\n",
    "    \"sort_indexes\": [32],\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Test activations_all endpoint\n",
    "base_url = \"http://127.0.0.1:5000\"\n",
    "url = f\"{base_url}/activations-all\"\n",
    "\n",
    "payload = {\n",
    "    \"text\": \" damn\",\n",
    "    \"model\": sae_1.cfg.model_name + \"-it\",\n",
    "    \"source_set\": \"gemmascope-mlp-65k\",\n",
    "    \"selected_layers\": [\"0-gemmascope-mlp-65k\"],\n",
    "    \"secret\": \"secret\",\n",
    "    \"sort_indexes\": [],\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "response.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
