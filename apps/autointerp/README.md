#### neuronpedia üß†üîç autointerp server

- [repo status](#repo-status)
- [what this is](#what-this-is)
- [simple non-docker setup](#simple-non-docker-setup)
- [some docker commands for reference (outdated, needs fixing)](#some-docker-commands-for-reference-outdated-needs-fixing)

## repo status

- we haven't had as much time to work on this, but we'd like to collaborate with eleuther to add more explainers, scorers, and include the openai auto-interp types.
- it would be fantastic to standardize on auto-interp formats (eg an `explainerType` should have xyz fields, a `scorerType` should have abc fields, etc)

## what this is

auto-interp explanations and scoring, using eleutherAI's [delphi](https://github.com/EleutherAI/delphi) (formerly `sae-auto-interp`)

as much as possible we try to use classes/types from the `packages/python/neuronpedia-autointerp-client`, which is autogenerated from the openapi spec under `openapi/schemas/openapi/autointerp-server.yaml`

> ‚ö†Ô∏è **warning:** this is draft documentation. we expect to either have better readmes or use a hosted documentation website.

> ‚ö†Ô∏è **warning:** the eleuther embedding scorer uses an embedding model only supported on CUDA (it won't work on mac mps or cpu)

> ‚ö†Ô∏è **warning:** this doesn't use poetry because there is some conflict with using poetry and the `xformers` library that we have yet to resolve [#9](https://github.com/hijohnnylin/neuronpedia/issues/9)

## simple non-docker setup

1. `python -m venv .venv`

2. `source .venv/bin/activate`

3. `pip install -e ".[dev]"`

4. launch local server

   ```
   # no auto-reload
   uvicorn server:app --host 0.0.0.0 --port 5003 --workers 1
   # with auto-reload
   uvicorn server:app --host 0.0.0.0 --port 5003 --workers 1 --reload
   ```

5. if you are making changes to the openapi spec (new/updated endpoints) and want to test those changes locally, generate your client and use the following command to point to the local autointerp client:
   `pip install -e ../../packages/python/neuronpedia-autointerp-client`

## some docker commands for reference (outdated, needs fixing)

likely we will just have these instructions in the root directory `readme` instead, and manual builds should happen the same way as we do it for `inference`.

build the image from root directory

```
# cpu
docker build --platform=linux/amd64 -t neuronpedia-autointerp:cpu -f apps/autointerp/Dockerfile --build-arg BUILD_TYPE=nocuda .

# gpu
docker build --platform=linux/amd64 -t neuronpedia-autointerp:gpu -f apps/autointerp/Dockerfile --build-arg BUILD_TYPE=cuda .
```

Push the image to the registry (using google cloud here)

```
# tag + push cpu
docker tag neuronpedia-autointerp:cpu gcr.io/$(gcloud config get-value project)/neuronpedia-autointerp:cpu
docker push gcr.io/$(gcloud config get-value project)/neuronpedia-autointerp:cpu

# tag + push gpu
docker tag neuronpedia-autointerp:gpu gcr.io/$(gcloud config get-value project)/neuronpedia-autointerp:gpu
docker push gcr.io/$(gcloud config get-value project)/neuronpedia-autointerp:gpu
```

Run the container locally

```
# replace SECRET with your own value - it must match what webapp knows, or auth will fail

# cpu
docker run -p 5003:5003 -e SECRET=[SECRET] neuronpedia-autointerp:cpu poetry run autointerp

# gpu
docker run --gpus all -p 5003:5003 -e SECRET=[SECRET] neuronpedia-autointerp:gpu poetry run autointerp
```
